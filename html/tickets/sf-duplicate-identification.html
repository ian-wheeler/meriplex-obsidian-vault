<h1 id="identifying-salesforce-duplicate-accounts">Identifying Salesforce Duplicate Accounts</h1>
<ol type="1">
<li>Login to Workbench <a href="https://workbench.developerforce.com/login.php">Workbench (developerforce.com)</a></li>
<li>Navigate to <a href="https://workbench.developerforce.com/query.php">Workbench: SOQL Query (developerforce.com)</a></li>
<li>Run the <code>all_accounts</code> SOQL Query</li>
</ol>
<h2 id="soql-query-all_accounts">SOQL Query <code>all_accounts</code></h2>
<p>This SOQL query retrieves a partial dataset for all <code>Account</code> objects from Salesforce, excluding the <code>BillingAddress</code> and <code>ShippingAddress</code> fields. These fields are compound data fields that contain detailed address information. Including them in the query results in the error message:</p>
<pre><code>Failed: InvalidBatch: Failed to process query: FUNCTIONALITY_NOT_ENABLED: Selecting compound data not supported in Bulk Query.&quot;</code></pre>
<p>This limitation is due to the Salesforce platformâ€™s restriction on selecting compound data in bulk queries.</p>
<pre class="soql"><code>SELECT AccountSource,Active_Status__c,AnnualRevenue,At_Risk_MRR__c,At_Risk__c,BillingCity,BillingCountry,BillingPostalCode,BillingState,BillingStreet,Company_ID__c,Company_Rec_ID__c,Description,GCC_High_Referral_Partner__c,Id,Industry,IsDeleted,Market__c,MasterRecordId,Name,Non_solicitation_Agreement__c,ParentId,Phone,ShippingCity,ShippingCountry,ShippingPostalCode,ShippingState,ShippingStreet,Status__c,Sub_Source__c,Territory__c,Type,Website FROM Account WHERE IsDeleted = false</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co"># Load the SF_AllAccounts sheet into a DataFrame</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>file_path <span class="op">=</span> <span class="st">&#39;/content/cw_sf_data.xlsx&#39;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>df <span class="op">=</span> pd.read_excel(file_path, sheet_name<span class="op">=</span><span class="st">&#39;SF_Accounts&#39;</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="co"># Define the list of address fields to check for duplicates</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>address_fields <span class="op">=</span> [<span class="st">&#39;BillingCity&#39;</span>, <span class="st">&#39;BillingCountry&#39;</span>, <span class="st">&#39;BillingPostalCode&#39;</span>, <span class="st">&#39;BillingState&#39;</span>, <span class="st">&#39;BillingStreet&#39;</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="co"># Drop rows where any of the address fields is NaN before checking for duplicates</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>df_cleaned <span class="op">=</span> df.dropna(subset<span class="op">=</span>address_fields, how<span class="op">=</span><span class="st">&#39;any&#39;</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="co"># Sort the DataFrame based on the address fields to group duplicates together</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a>df_sorted <span class="op">=</span> df_cleaned.sort_values(by<span class="op">=</span>address_fields)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a><span class="co"># Identify the indices of the rows with duplicate address fields</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>duplicate_indices <span class="op">=</span> df_sorted.duplicated(subset<span class="op">=</span>address_fields, keep<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a><span class="co"># Filter the sorted DataFrame to only include the duplicate row</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a>duplicates_grouped <span class="op">=</span> df_sorted[duplicate_indices]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a><span class="co"># Save the grouped duplicates to a new Excel file</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a>output_file <span class="op">=</span> <span class="st">&#39;/content/sf_duplicate_accounts_by_address.xlsx&#39;</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true"></a>duplicates_grouped.to_excel(output_file, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true"></a>output_file</span></code></pre></div>
